# Data Science Portfolio
### By Antonio Manuel González Ferreira

All materials in the present are part of the developing of the project under the name Anomaly Detection in Maritime Vessels Traffic done in cooperation with Saab Technologies. In this repository you will find my contributions to the  project during the minor in Applied Data Science @ The Hague University of Applied Data Sciences. 

All the code generated during the semester has been writen in _Python_, more precisely using _Jupyter Notebooks_. Hereunder you will find links to those in which I have taken part with explanations in more detail on the referenced files. Thus, this document serves as a index guideline.

_**Note**: One of the python packages used for visualizations preceding the final prototype is _iPyLeaflet_. The maps generated by this in-line geodata library are not available any more after having saved the notebook so some visualizations will be linked to a HTML file or a picture._

## Table of contents
   * [Table of contents](#table-of-contents)
   * [Introduction](#Introduction-to-the-project)
      * [Project's Value](#projects-value)
      * [What is an anomaly?](#what-is-an-anomaly)
   * [About the data](#about-the-data)
     * [Codebook](#codebook)
     * [Data Exploration](#data-exploration)
        * [iPyLeaflet & iPyWidgets](#ipyleaflet-and-ipywidgets)
        * [Visualizations & data preproccessing](#visualizations-and-data-preproccessing)
   * [About the Models](#about-the-models)
     * [Dynamic Markov Models](#dynamic-markov-models)
        * [About DMM](#about-dmm)
        * [DMM results](#dmm-results)
     * [Recurrent Neural Network](#recurrent-neural-network)
     * [Density Map Models]
     * [KNN adaptation to unsupervised learning]
   
   
        
## Introduction to the project
### Project's Value
&emsp;Over the past decades there has been a significant rise in maritime traffic. This means that managing this traffic is an increasingly difficult task due to the amount of data that needs to be processed. Unfortunately, by the time the sea routes are becoming more congested, most of the managing methods and prococols (traditionally hand-operated) are overwhelming operators arising a descent on its efficiency.

&emsp;Data science has a lot to do to improve the maritime services and vigilance and Anomaly Detection is a really valuable field to put research efforts, primarily because of a fact: Unguarded maritime traffic can lead to ilegal trade, fish poaching, collissions that can supose high costs and a long list preventable troubles on harbours.</p>

&emsp;There is a lot of literature on the topic but as it is a highly transversal issue, the methods to detect anomalies are not standarized and depend much about the field to which we want to apply it.

### What is an anomaly?

&emsp;In daily terms a maritime vessel anomaly can be defined as a deviation from the normal behavior of a ship under previously studied circunstances. This delimitation of the subject can seem to be not accurate enough but that is why this is an up-to-date topic. Nevertheless and with this subjacent concept present, during the literature study phase of the project we could reach more mathematical clarity arround the concept. According to the product owner, the current definition about anomaly detection in Saab is standarized as:

1. Three/four times out of the standard deviation when following a route.
2. Two times out of the standard deviation particularily in terms of speed.
3. Making sudden course changes/movements.

## About the data
### Codebook
&emsp;It a CSV file with 10 columns. The file is 3.9GBs and contains 4.2 million rows. The origin of the data is AIS data captured by Saab, integrated with their own radar data.

&emsp;A codebook with some basic details about the data can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/Codebook.ipynb).

### Collecting the data

&emsp;The data we have been dealing with is AIS data collected by Saab radars nearby Hong Kong's harbour. AIS stands for Automatic Identification System, preprocessing the data is a tough task as traducing the information received by the radars to useful information involves filtering noise and figure out what is going on under some spetial circunstances such as two ships overlaping in front of a radar with different sizes, speeds, etc. That task was given done to us by Saab tracking specialists. Anyway during the project we found some issues with the data. To see some of those deffects on the data see the section [Visualization](#visualization).

### Data exploration

&emsp;At the beginning of the project we had some trouble to get in touch with the data, as we are dealing with confidential data, we were involved in a lot of burocracy delays within the first weeks due to the signing and processing of the NDA's and moving the data to the University servers. 
&emsp;It was not until week 6 when we received the real data. During that period I had a lot of curiosity about the data so I tried to encourage the team to use a Kaggle dataset to start preparing some visualizations as AIS data is the most common way to identify ships all arround the worls and the scalability of the work that we would do was high. With that on mind, we started doing some visualizations on the australian dataset that can be followed [here](https://www.operations.amsa.gov.au/Spatial/DataServices/DigitalData). As can be seen in the link, the information is directly provided by the Australian Government and its free to access it. 
&emsp;Finally we got the access to the data and, as planned, it was really easy to change the datatype.
&emsp;To start hands-on the data I made several simple plots such as the ones present in [this notebook](https://github.com/antoniomgf1998/Portfolio/blob/master/Statistics of the data.ipynb)

#### iPyLeaflet and iPyWidgets
&emsp;I have been in charge of all the visualizations during the project until developing the prototype when our needs of changed to offline mapping. On the first days I started looking for a package on the internet to visualize our data. Finally I chose [ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/).

> ipyleaflet is an interactive widgets library, it is based on ipywidgets. This means that everything in ipyleaflet (e.g. the Map, TileLayers, Markers…) is interactive: you can dynamically update attributes from Python or from the Notebook interface.

&emsp;One of the best points of this package is the interactivity that can be established easily with [iPyWidgets](https://ipywidgets.readthedocs.io/en/latest/) to dinamically change the visualizations inline on our Jupyter notebooks.

&emsp;The problem with this two libraries when we started to develop the prototype was the computational time it lasted to plot ships on time instead of their whole routes but It was not until almost the end of the project when we moved to pure JS and JSON files for offline plotting (which was also a requirement of the product owner). The timings were extremely increasing when we tried to plot steps on the sorted datetime feature. This issue is due to the fact that ipyleaflet works concatenating layers so that all elements ploted on a map is represented on a different layer so if instead of ploting each entire ship route at one separated layer, we try to plot ships on timesteps the amount of layers increases exponentially taking into account that we have information from every ship every 3 seconds.

#### Visualizations and data preprocessing

&emsp;All the visualizations during the project were evolutions of the presented examples hereunder.
&emsp;Before Stephan came up with the idea of developing our own library, I started creating detailed Jupyter notebooks with my functions so that the rest of the group could consult any doubt, [here](https://github.com/antoniomgf1998/Portfolio/blob/master/Data_visualization_functions.md) is the link to the notebook I started with that goal.

_**Note**: Most of the visualizations are too big to get uploaded to this repo as HTML. In substitution, I have divided this section into the link to hthe notebook and mostly just a screenshot of the results produced by it._

##### 1. **Example of visualization on Australia Dataset**

&emsp;**[Static visualization on australian dataset with random color and detail popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/data_visualization_static.ipynb)**

###### 1.1. [HTML Random colored ships with details as clickable popups (result)](https://github.com/antoniomgf1998/Portfolio/blob/master/australia_50_random_color_visualization.html)

##### 2. **Example of visualization on entire Hong Kong Dataset**

&emsp;**[Dynamic visualization on Hong Kong dataset with random color and detail popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/data_visualization_dynamic-HK_whole_dataset.ipynb)**

###### 2.1. Visualization of entire dataset Hong Kong

&emsp;In the image shown below the noise on the datased without preprocessing can clearly been seen. This was one of the first issues we faced on the first weeks. The ships with abnormal behaviour shuch as the one coloured in blue (covering a great part of the map) were taken out from the dataset.

&emsp;But It all did not finish with the removal of some ships. We noticed that not only one ship was noise, that was the main issue in preproccessing: looking for noise ships, get their mmsi's (Ship ID) and take them out.

![HTML Random colored ships with details as clickable popups HK dataset (result)](https://github.com/antoniomgf1998/Portfolio/blob/master/random_color_plot_whole_area_HK.PNG)

##### 3. **Dynamic Visualization with slider on Hong Kong reduced area data**

&emsp;By the time our first trials with models were evolving the computational time spent on running some code started growing exponentially. Thus, we agreed to focus on just one part of the given area (I will refer to it as reduced area from now on). This part of the map was chosen with the product owner because of its huge traffic and assumable timings while running our scripts after testing them out within it.

&emsp;**[Dynamic visualization on Hong Kong - reduced area dataset with random color and details popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/data_visualization_dynamic.ipynb)**

###### 3.1. Density map

&emsp;Visualization of the overlaping routes to identify the most concurrent routes (The darker --> The more populated path). All routes were plotted with a transparency of 0.5.

![Density Map](https://github.com/antoniomgf1998/Portfolio/blob/master/density_map_plot_reduced_area.PNG)

###### 3.2. Testing visualize_by_id function results (prototype v.0.)

This function was aimed to visualize in red, green and orange all the ship routes. So, whenever we got a model to work we could just specify what ships were anomalous (red), suspicious to be anomalous (orange) and within normality (green). The image below does not have any model supporting its conclusions, it is just a test.

![Preparing the Prototype (initial version)](https://github.com/antoniomgf1998/Portfolio/blob/master/prototype_v1.PNG)

## Models
&emsp;During the semester we have tried several models to detect anomalies in our dataset

### Dynamic Markov Models (DMM)

#### About DMM

&emsp;Inspired by [this](https://www.sciencedirect.com/science/article/pii/S0020025517307302?via%3Dihub) paper I started my learning on markov Models after the preprocessing phase to start having some results and notions about the main issues when trying to implement models to our goals with the Hong Kong data.
&emsp;Markov Models, framed in probability theory, is a stochastic model to model randomly changing mathematical systems. For example, given a system with some defined states over which elements flow, if we correctly define the probability of moving from one state to another according to a training sample, we will be able to detect transitions that _a priori_ would be rare (rated with low probability) or in other words: **anomalies**.
&emsp;The essential information about Markov Models and our project can be found in [this](https://github.com/antoniomgf1998/Portfolio/blob/master/Dynamic_Markov_Models.pdf) presentation I prepared to show the rest of the group how dynamic markov models could be applied to our project.
&emsp;Summarizing the main idea of the presentation that supports the fact that Dynamic Markov fit with our objective (detecting anomalies) I must say that taking into account that the trickiest part of detecting an anomaly is that features must not be studied in a separated way but united, for example speed and length, the two features with which the implementation of the markov model was done have a big relation between following the idea that high speeds are more dangerous for big ships than for small ones so the same speed for a big ship and for a small one might be an anomaly and a normal behaviour respectively under certain circunstances.

&emsp;Another important idea about the implementation, related to the partitions that must be made in the features to define the borders between states, is the fact that our dataset is more populated with small ships than with large ones. This is important because if we set the partition borders linearily, there are surely going to appear some states with no information, see the image below. The solution given to this problem is using percentiles to set the borders.

> E.g.: Given a feature whose punctual values form the set <a href="https://www.codecogs.com/eqnedit.php?latex=X" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X" title="X" /></a> from which we know that <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\forall&space;x_i&space;\epsilon&space;X,&space;0&space;<&space;x_i&space;\le&space;50" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\forall&space;x_i&space;\epsilon&space;X,&space;0&space;<&space;x_i&space;\le&space;50" title="\forall x_i \epsilon X, 0 < x_i \le 50" /></a> but the 70% of punctual values is lower than 20. If we partitionate this feature to define 5 states linearily...<br>
> State 1: from 0 to 10<br>
> State 2: from 10 to 20<br>
> **State 3: from 20 to 30**<br>
> **State 4: from 30 to 40**<br>
> **state 5: from 40 to 50**<br>
> Then states **3, 4 and 5** would be empty.<br>
> If, instead we define the borders like:<br>
> State 1: values from 0 to the 20th percentile<br>
> State 2: values from the 20th percentile to the 40th percentile<br>
> State 3: values from the 40th percentile to the 60th percentile<br>
> State 4: values from the 60th percentile to the 80th percentile<br>
> state 5: values from the 80th percentile to 50<br>
> **The problem is fixed: all states have the same amount of data and there are no empty states, see the image**

![Comparison between linear and percentile partitions](https://github.com/antoniomgf1998/Portfolio/blob/master/linear-quantile-partitions_DMM.png)

#### DMM Results

&emsp;The final version of the DMM implementation to speed and length in our data can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/DMM_final_version.ipynb)

&emsp;Hereunder some of the outputs can be shown:

###### 1. DMM Anomalies map

![Anomalies map DMM](https://github.com/antoniomgf1998/Portfolio/blob/master/anomaliesmapDMM.PNG)

###### 2. Some speed-vs-time anomalies plots

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM_anomalyex1.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM_anomalyex2.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM_anomalyex3.png" width="33%">

&emsp;The results were apparently not bad even though a little noisy, most of the anomalies detected by it are visually reasonable but the evaluation of most of this anomalies requires both probably more data to cover the theoreticall simplicity of the model and more field knowledge than the one our product owner has and the one we could acquire within this months of work(requires experience in evaluating types of ships, information about the highest speed per individual ship) so it would last a lot of time to get the model evaluated by us with a not assumable uncertainty.

### Recurrent Neural Networks

&emsp;A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. From this point of view, RNN's fit perfectly to our kind of data since all 

&emsp;From the very beginning, our product owner encouraged us to learn about RNN's
