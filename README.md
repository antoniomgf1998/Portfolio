# Portfolio
## Anomaly Detection in Maritime Vessel's Traffic
###### By Antonio Manuel González Ferreira

All materials in the present document are part of the developing of the project under the name Anomaly Detection in Maritime Vessels Traffic done in cooperation with Saab Technologies. In this repository you will find my contributions to the  project during the minor in Applied Data Science @ The Hague University of Applied Data Sciences. 

All the code generated during the semester has been writen in _Python_, more precisely using _Jupyter Notebooks_. Hereunder you will find links to those in which I have taken part with explanations in more detail.

_**Note: One of the python packages used for visualizations preceding the final prototype is _iPyLeaflet_. The maps generated by this in-line geodata library are not available any more after having saved the notebook so some visualizations will be shown as a screenshot.**_

## Table of contents
   * [Table of contents](#table-of-contents)
   * [Introduction to the Project](#introduction-to-the-project)
      * [DataCamp Courses](#datacamp-courses)
      * [Communication](#communication)
      * [Developing Methodology](#developing-methodology)
      * [Task Definition and Project's value](#task-definition-and-projects-value)
         * [Research Questions](#research-questions)
            * [Main Question](#main-question)
            * [Subquestions](#subquestions)
      * [What is an anomaly?](#what-is-an-anomaly)
   * [About Data](#about-the-data)
     * [Codebook](#codebook)
     * [Collecting the data](#collecting-the-data)
     * [Data exploration](#data-exploration)
        * [iPyLeaflet & iPyWidgets](#ipyleaflet-and-ipywidgets)
        * [Visualizations & data preprocessing](#visualizations-and-data-preprocessing)
   * [Models](#models)
     * [Dynamic Markov Models](#dynamic-markov-models)
        * [About DMM](#about-dmm)
        * [DMM results](#dmm-results)
     * [Recurrent Neural Network](#recurrent-neural-network)
        * [About RNN](#about-rnn)
        * [RNN Results](#rnn-results)
     * [Simple Neural Networks and fake Anomalies](#simple-neural-networks-and-fake-anomalies)
        * [About NN approach](#about-nn-approach)
        * [NN results](#nn-results)
     * [Density Map Models](#density-map-models)
        * [About Density Maps](#about-density-maps)
        * [Density Maps Results](#density-maps-results)
     * [KNN adaptation to unsupervised learning](#knn-adaptation-to-unsupervised-learning)
        * [About unsupervised KNN](#about-unsupervised-knn)
        * [Unsupervised KNN results](#unsupervised-knn-results)
   * [Prototype](#prototype)
   * [Paper](#paper)
   * [Reflection](#reflection)
      * [Reflection on own contribution to the project](#reflection-on-own-contribution-to-the-project)
      * [Reflection on own learning objectives](#reflection-on-own-learning-objectives)
      * [Evaluation on the group project as a whole](#evaluation-on-the-group-project-as-a-whole)

   * [Conclussion](#conclussion)
   
        
## Introduction to the project

### DataCamp Courses
&emsp;I completed succesfully all the assignments on the datacamp courses. Even though I already had python experience, It was constructive and served me to learn useful information about code optimization with Pandas and numpy and about graphing in matplotlib.
![](https://github.com/antoniomgf1998/Portfolio/blob/master/DATACAMP/DATACAMP1.PNG)![](https://github.com/antoniomgf1998/Portfolio/blob/master/DATACAMP/DATACAMP2.PNG)

### Communication
&emsp;In terms of communication, I have contributed to all of the presentations preparation and presented 4 times along the course. One of our objectives as a group was presenting everyone in cycle for a better overview of all team members.

> "If you can't explain it simply, you don't understand it well enough.", A. Einstein

[Folder with all the presentations](https://github.com/antoniomgf1998/Portfolio/blob/master/WEEKLY_PRESENTATIONS)

&emsp;Presentations was not only a medium of communication at a school level, as can be seen in the folder, we have also been preparing presentations to show at Saab's offices for the sprint retrospective and planning.

&emsp;As we all have a look more frequently at our personal gmail account that the school, which we have redirected to gmail, we have been working on documentation using Google Drive and Google Docs.

&emsp;For task communications, we used trello which fits at best to our methodology requirement of a digital/analogical board.

### Developing Methodology
&emsp;The developing methodology we have used during the minor is SCRUM. SCRUM is specially efficient for groups with few people such as our (6 people) and is one of the methodologies most commonly used in the work world nowadays. Our daily standups have taken place at school at 10:00 am. and our sprint duration on average has been two weeks, this has to do with the fact that we met at Apeldorn (Saab Technologies offices) to plan the upcoming sprints and evaluate our performance on the previous one with Jordi, our produc owner. This sprint reviews have been irregular because some issues have made us change the sprint review and planning to a skype meeting in the meidle of the planned one.
&emsp;As said in the previous point, we have been using Trello Board for SCRUM assignments communications.

### Task Definition and Projects Value
&emsp;Over the past decades there has been a significant rise in maritime traffic. This means that managing this traffic is an increasingly difficult task due to the amount of data that needs to be processed. Unfortunately, by the time the sea routes are becoming more congested, most of the managing methods and prococols (traditionally hand-operated) are overwhelming operators arising a descent on its efficiency.

&emsp;Data science has a lot to do to improve the maritime services and vigilance and Anomaly Detection is a really valuable field to put research efforts, primarily because of a fact: Unguarded maritime traffic can lead to ilegal trade, fish poaching, collissions that can supose high costs and a long list preventable troubles on harbours.</p>

&emsp;There is a lot of literature on the topic but as it is a highly transversal issue, the methods to detect anomalies are not standarized and depend much about the field to which we want to apply it.

#### Research Questions

###### Main question
&emsp;**Is it possible to create an algorithm that can read the behaviour of ships in real-time and label certain ships as an anomaly?**

###### Subquestions

&emsp;1. What is maritime traffic?
&emsp;2. What is an anomaly?
&emsp;3. What methods can be used for detecting anomalies?


### What is an anomaly?

&emsp;This is the main question when it comes to jargon in this project. In daily terms a maritime vessel anomaly can be defined as a deviation from the normal behavior of a ship under previously studied circunstances. This delimitation of the subject can seem to be not accurate enough but that is why this is an up-to-date topic. Nevertheless and with this subjacent concept present, during the literature study phase of the project we could reach more mathematical clarity arround the concept. According to the product owner, the current definition about anomaly detection in Saab is standarized as:

1. Three/four times out of the standard deviation when following a route.
2. Two times out of the standard deviation particularily in terms of speed.
3. Making sudden course changes/movements.

## About the data
### Codebook
&emsp;It a CSV file with 10 columns. The file is 3.9GBs and contains 4.2 million rows. The origin of the data is AIS data captured by Saab, integrated with their own radar data.

&emsp;A codebook with some basic details about the data can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/Codebook.md).

### Collecting the data

&emsp;The data we have been dealing with is AIS data collected by Saab radars nearby Hong Kong's harbour. AIS stands for Automatic Identification System, preprocessing the data is a tough task as traducing the information received by the radars to useful information involves filtering noise and figure out what is going on under some spetial circunstances such as two ships overlaping in front of a radar with different sizes, speeds, etc. That task was given done to us by Saab tracking specialists. Anyway during the project we found some issues with the data. To see some of those deffects on the data see the section [Visualization](#visualization).

### Data exploration

&emsp;At the beginning of the project we had some trouble to get in touch with the data, as we are dealing with confidential data, we were involved in a lot of burocracy delays within the first weeks due to the signing and processing of the NDA's and moving the data to the University servers. 
&emsp;It was not until week 6 when we received the real data. During that period I had a lot of curiosity about the data so I tried to encourage the team to use a Kaggle dataset to start preparing some visualizations as AIS data is the most common way to identify ships all arround the worls and the scalability of the work that we would do was high. With that on mind, we started doing some visualizations on the australian dataset that can be followed [here](https://www.operations.amsa.gov.au/Spatial/DataServices/DigitalData). As can be seen in the link, the information is directly provided by the Australian Government and its free to access it. 
&emsp;Finally we got the access to the data and, as planned, it was really easy to change the datatype.
&emsp;To start hands-on the data I made several simple plots such as the ones present in [this notebook](https://github.com/antoniomgf1998/Portfolio/blob/master/Statistics_of_the_data.ipynb)

#### iPyLeaflet and iPyWidgets
&emsp;I have been in charge of all the visualizations during the project until developing the prototype when our needs of changed to offline mapping. On the first days I started looking for a package on the internet to visualize our data. Finally I chose [ipyleaflet](https://ipyleaflet.readthedocs.io/en/latest/).

> ipyleaflet is an interactive widgets library, it is based on ipywidgets. This means that everything in ipyleaflet (e.g. the Map, TileLayers, Markers…) is interactive: you can dynamically update attributes from Python or from the Notebook interface.

&emsp;One of the best points of this package is the interactivity that can be established easily with [iPyWidgets](https://ipywidgets.readthedocs.io/en/latest/) to dinamically change the visualizations inline on our Jupyter notebooks.

&emsp;The problem with this two libraries when we started to develop the prototype was the computational time it lasted to plot ships on time instead of their whole routes but It was not until almost the end of the project when we moved to pure JS and JSON files for offline plotting (which was also a requirement of the product owner). The timings were extremely increasing when we tried to plot steps on the sorted datetime feature. This issue is due to the fact that ipyleaflet works concatenating layers so that all elements ploted on a map is represented on a different layer so if instead of ploting each entire ship route at one separated layer, we try to plot ships on timesteps the amount of layers increases exponentially taking into account that we have information from every ship every 3 seconds.

#### Visualizations and data preprocessing

&emsp;All the visualizations during the project were evolutions of the hereunder presented examples done by me.
&emsp;Before Stephan came up with the idea of developing our own library, I started creating detailed Jupyter notebooks with my functions so that the rest of the group could consult any doubt, [here](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/Data_visualization_functions.md) is the link to the notebook I started with that goal.

_**Note**: Most of the visualizations are too big to get uploaded to this repo as HTML. In substitution, I have divided this section into the link to hthe notebook and mostly just a screenshot of the results produced by it._

##### 1. **Example of visualization on Australia Dataset**

&emsp;**[Static visualization on australian dataset with random color and detail popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/data_visualization_static.ipynb)**

###### 1.1. [HTML Random colored ships with details as clickable popups (result)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/australia_50_random_color_visualization.html)

##### 2. **Example of visualization on entire Hong Kong Dataset**

&emsp;**[Dynamic visualization on Hong Kong dataset with random color and detail popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/data_visualization_dynamic-HK_whole_dataset.ipynb)**

###### 2.1. Visualization of entire dataset Hong Kong

&emsp;In the image shown below the noise on the datased without preprocessing can clearly been seen. This was one of the first issues we faced on the first weeks. The ships with abnormal behaviour shuch as the one coloured in blue (covering a great part of the map) were taken out from the dataset.

&emsp;But It all did not finish with the removal of some ships. We noticed that not only one ship was noise, that was the main issue in preproccessing: looking for noise ships, get their mmsi's (Ship ID) and take them out.

![HTML Random colored ships with details as clickable popups HK dataset (result)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/random_color_plot_whole_area_HK.PNG)

##### 3. **Dynamic Visualization with slider on Hong Kong reduced area data**

&emsp;By the time our first trials with models were evolving the computational time spent on running some code started growing exponentially. Thus, we agreed to focus on just one part of the given area (I will refer to it as reduced area from now on). This part of the map was chosen with the product owner because of its huge traffic and assumable timings while running our scripts after testing them out within it.

&emsp;**[Dynamic visualization on Hong Kong - reduced area dataset with random color and details popups (notebook)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/data_visualization_dynamic.ipynb)**

###### 3.1. Visualizing Traffic Density

&emsp;Visualization of the overlaping routes to identify the most concurrent routes (The darker --> The more populated path). All routes were plotted with a transparency of 0.5.

![Density Map](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/density_map_plot_reduced_area.PNG)

###### 3.2. Testing visualize_by_id function results (prototype v.0.)

This function was aimed to visualize in red, green and orange all the ship routes. So, whenever we got a model to work we could just specify what ships were anomalous (red), suspicious to be anomalous (orange) and within normality (green). The image below does not have any model supporting its conclusions, it is just a test.

![Preparing the Prototype (initial version)](https://github.com/antoniomgf1998/Portfolio/blob/master/VISUALIZATIONS/prototype_v1.PNG)

## Models
&emsp;During the semester we have tried several models to detect anomalies in our dataset

### Dynamic Markov Models

#### About DMM

&emsp;Inspired by [this](https://www.sciencedirect.com/science/article/pii/S0020025517307302?via%3Dihub) paper I started my learning on markov Models after the preprocessing phase to start having some results and notions about the main issues when trying to implement models to our goals with the Hong Kong data.
&emsp;Markov Models, framed in probability theory, is a stochastic model to model randomly changing mathematical systems. For example, given a system with some defined states over which elements flow, if we correctly define the probability of moving from one state to another according to a training sample, we will be able to detect transitions that _a priori_ would be rare (rated with low probability) or in other words: **anomalies**.
&emsp;The essential information about Markov Models and our project can be found in [this](https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/Dynamic_Markov_Models.pdf) presentation I prepared to show the rest of the group how dynamic markov models could be applied to our project.
&emsp;Summarizing the main idea of the presentation that supports the fact that Dynamic Markov fit with our objective (detecting anomalies) I must say that taking into account that the trickiest part of detecting an anomaly is that features must not be studied in a separated way but united, for example speed and length, the two features with which the implementation of the markov model was done have a big relation between following the idea that high speeds are more dangerous for big ships than for small ones so the same speed for a big ship and for a small one might be an anomaly and a normal behaviour respectively under certain circunstances.

&emsp;Another important idea about the implementation, related to the partitions that must be made in the features to define the borders between states, is the fact that our dataset is more populated with small ships than with large ones. This is important because if we set the partition borders linearily, there are surely going to appear some states with no information, see the image below. The solution given to this problem is using percentiles to set the borders.

> E.g.: Given a feature whose punctual values form the set <a href="https://www.codecogs.com/eqnedit.php?latex=X" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X" title="X" /></a> from which we know that <br> <a href="https://www.codecogs.com/eqnedit.php?latex=\forall&space;x_i&space;\epsilon&space;X,&space;0&space;<&space;x_i&space;\le&space;50" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\forall&space;x_i&space;\epsilon&space;X,&space;0&space;<&space;x_i&space;\le&space;50" title="\forall x_i \epsilon X, 0 < x_i \le 50" /></a> but the 70% of punctual values is lower than 20. If we partitionate this feature to define 5 states linearily...<br>
> State 1: from 0 to 10<br>
> State 2: from 10 to 20<br>
> **State 3: from 20 to 30**<br>
> **State 4: from 30 to 40**<br>
> **state 5: from 40 to 50**<br>
> Then states **3, 4 and 5** would be empty.<br>
> If, instead we define the borders like:<br>
> State 1: values from 0 to the 20th percentile<br>
> State 2: values from the 20th percentile to the 40th percentile<br>
> State 3: values from the 40th percentile to the 60th percentile<br>
> State 4: values from the 60th percentile to the 80th percentile<br>
> state 5: values from the 80th percentile to 50<br>
> **The problem is fixed: all states have the same amount of data and there are no empty states, see the image**

![Comparison between linear and percentile partitions](https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/linear-quantile-partitions_DMM.png)

#### DMM Results

&emsp;The final version of the DMM implementation to speed and length in our data can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/DMM_final_version.ipynb)

&emsp;Hereunder some of the outputs can be shown:

###### 1. DMM Anomalies map

![Anomalies map DMM](https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/anomaliesmapDMM.PNG)

###### 2. Some speed-vs-time anomalies plots

&emsp;As I was dealing with not-labeled data, I had to make out how to evaluate the model. In this case, I started ploting for each whole route analized speed-on-time plots to see if anomalies detected visually match with weird movements such as huge differences between two points. Hereunder some images of this plots.

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/DMM_anomalyex1.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/DMM_anomalyex2.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DMM/DMM_anomalyex3.png" width="33%">



&emsp;The results were apparently not bad even though a little noisy, most of the anomalies detected by it are visually reasonable but the evaluation of most of this anomalies requires both probably more data to cover the theoreticall simplicity of the model and more field knowledge than the one our product owner has and the one we could acquire within this months of work(requires experience in evaluating types of ships, information about the highest speed per individual ship) so it would last a lot of time to get the model evaluated by us with a not assumable uncertainty.

### Recurrent Neural Network

#### About RNN

&emsp;A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. From this point of view, RNN's fit perfectly to our kind of data since all ships routes can be interpreted as a temporal sequence. The point where this model fits to our purpose is that context matters on what the behaviour of the ship should be. From the very beginning, our product owner encouraged us to learn about RNN's because of all this facts so the subject of RNN's was really present in the phase of the literature study.

&emsp; The approach with RNN, as the data we have is not labelled and we need ground truth to train our model (RNN is a supervised type of model), was trying to predict the next step after having trained with sequences of movements. This was my suggestion at the beginning of the implementation, otherwise with not labelled data It would not make sense to apply this type of algorithm and our product owner really wanted us to explore RNN's as a solution.

&emsp;Going a bit more in detail about the implementation of this RNN, we tried both with LSTM denses and with GRU denses, which are the main units used when it comes to RNN's:

* 1. LSTM stands for Long-Short term memory and the remarcable point of this units is the capability of take into account the context (time) of a given part of the input contrary to classical feedforward models trained with backpropagation.

* 2. GRU stands for Gated Recurrent Unit, which are units like LSTM with forget gate to enforce the capability of forgeting what it determines not useful information.

#### RNN Results

&emsp;The final version of the RNN implementation in our data can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/LSTM/RNN_V4_predict_route+visualize_predictions.ipynb)

&emsp;By the time we started our RNN implementation I was busy and still in charge of the DMM approach so I missed the first implementations which were focused in trying to predict just one ship route learning from small segments of it, getting some "acceptable results" (mse <= 0.001). This value of loss is not conclusive taking into account that an error of 0.00001 in (lat,lon) coordinates means 100m of real errors in our predictions, and this was only the first phase (we were pretending to move from positional prediction to other features predictions). Even though the data is scaled and mse cannot be directly compared to real error, I made some calculations that clarified there was something unexpectedly wrong going on.
> <a href="https://www.codecogs.com/eqnedit.php?latex=Let\:(X,Y)\epsilon(\mathbb{R}^{2n}\times\mathbb{R}^{2})\:be\:the\:dataset\:and\:\gamma\epsilon\mathbb{R}^2&space;\:be\:our\:positional\:predictions\:\\where\:n=timesteps-1.\\\\&space;Defining\:as\:assumable\:that\:given\:(Y_i,\gamma_i),&space;|Y_i-\gamma_i|\le10^{-5}\:\:(10\:meters),\\&space;Which\:traduced\:to\:scaled\:data(X^s,Y^s,\gamma^s),\:as\:it\:reduces\:the\:order\:of\\\:magnitude\:10^2\:is:\\\\&space;|Y^s_i-\gamma^s_i|\le10^{-7}(=10^{-5}10^{-2})\\\\&space;Using\:MSE\:as\:cost\:function...\\&space;|Y^s_i-\gamma^s_i|\le10^{-7}\Rightarrow&space;(Y^s_i-\gamma^s_i)^2\le10^{-14}\Rightarrow&space;\sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le&space;N10^{-14}\Rightarrow&space;\\&space;\frac{1}{N}\sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le&space;10^{-14}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Let\:(X,Y)\epsilon(\mathbb{R}^{2n}\times\mathbb{R}^{2})\:be\:the\:dataset\:and\:\gamma\epsilon\mathbb{R}^2&space;\:be\:our\:positional\:predictions\:\\where\:n=timesteps-1.\\\\&space;Defining\:as\:assumable\:that\:given\:(Y_i,\gamma_i),&space;|Y_i-\gamma_i|\le10^{-5}\:\:(10\:meters),\\&space;Which\:traduced\:to\:scaled\:data(X^s,Y^s,\gamma^s),\:as\:it\:reduces\:the\:order\:of\\\:magnitude\:10^2\:is:\\\\&space;|Y^s_i-\gamma^s_i|\le10^{-7}(=10^{-5}10^{-2})\\\\&space;Using\:MSE\:as\:cost\:function...\\&space;|Y^s_i-\gamma^s_i|\le10^{-7}\Rightarrow&space;(Y^s_i-\gamma^s_i)^2\le10^{-14}\Rightarrow&space;\sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le&space;N10^{-14}\Rightarrow&space;\\&space;\frac{1}{N}\sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le&space;10^{-14}" title="Let\:(X,Y)\epsilon(\mathbb{R}^{2n}\times\mathbb{R}^{2})\:be\:the\:dataset\:and\:\gamma\epsilon\mathbb{R}^2 \:be\:our\:positional\:predictions\:\\where\:n=timesteps-1.\\\\ Defining\:as\:assumable\:that\:given\:(Y_i,\gamma_i), |Y_i-\gamma_i|\le10^{-5}\:\:(10\:meters),\\ Which\:traduced\:to\:scaled\:data(X^s,Y^s,\gamma^s),\:as\:it\:reduces\:the\:order\:of\\\:magnitude\:10^2\:is:\\\\ |Y^s_i-\gamma^s_i|\le10^{-7}(=10^{-5}10^{-2})\\\\ Using\:MSE\:as\:cost\:function...\\ |Y^s_i-\gamma^s_i|\le10^{-7}\Rightarrow (Y^s_i-\gamma^s_i)^2\le10^{-14}\Rightarrow \sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le N10^{-14}\Rightarrow \\ \frac{1}{N}\sum_{i=1}^{N}(Y^s_i-\gamma^s_i)^2\le 10^{-14}" /></a>

&emsp;Result from which we were too far (reaching from 10e-3 to 10e-4 on our validation MSE values).

&emsp;In the images below can be found the results from a model with a model consisting of a 150 LSTM layer, and an output layer trying to predict the next 20th position of a 19-size-sequence of points over different areas of the map.

    _**Note**: **Thick** line --> Real course._

    _Thin line --> Predictions starting from the penultimate position of the route._

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/LSTM/LSTM_visualization_1.PNG">
<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/LSTM/LSTM_visualization_2.PNG">

&emsp;The bad results and the uncertainty about where they were coming from led us to ask Jeroen for advise. With him, we reached the conclusion of trying out simpler models changing the inputs, see [Neural Networks](#neural-networks) section.

### Simple Neural Networks and fake Anomalies

###### About NN approach

&emsp;In this approach I have been involved partially at the beginning but not as in the rest of the models.

&emsp;Due to the problems with the LSTM-RNN approach, we tried to start predicting routes with a simpler model (without LSTM's). For this purpose I developed a function in [this notebook](https://github.com/antoniomgf1998/Portfolio/blob/master/NN/V8_translation_to_function.ipynb) to train multiple model architectures for evaluation tasks but I was not in charge of evaluating them. Even though that, not much better results were obtained. This made us change once again the approach with neural networks to classifying.

&emsp;But... how to classify if we do not have labeled data. The idea of this approach lays on generating realistic fake anomalies in a sufficient amount to enable a NN predict if a certain ship route is anomalous or not. The goal is not just classifying the artificial ones so the evaluation of this NN must be different from normal approaches to prediction with NN (that has been the main concept I have translated to the group members in charge of this approach, Akash and Dauwe).

&emsp;The other contribution to this approach has been a function to generate anomalies by rotating ship routes whose evidence I can not present as it was developed under a member's ID at the GPU server to which I do not have access.

###### NN results

&emsp;We reached some good metrics (90% acc) of the model firstly with non really realistic anomalies but when we improved our algorithm to generate more realistic ones, this values got lowed to 50-60%. Even though that, this approach is interesting but we did not have enough time to spend on it since It comes from the failure of others and we started working on it late. In further research I would focus more on the way fake anomalies are generated and I would introduce a huge ammount of anomalies so the probability of detecting real ones in the false positives in the confussion matrix would get higher.

### Density Map Models
#### About Density Maps

&emsp;The final version of the model derived from the density map and images explained below can be found [here](https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/Density_grid_points-rounding.ipynb)

&emsp;During the time we were stucked with the RNN, our product owner asked us for a certain task: obtain weighted graph (grid) representing the amount of ships that commonly cross nearby the nodes of it. I reached the result by developing the following notebook in which I provide various visualizations.

&emsp;One of the ideas that I am at most proud of when it comes to this developed model by me is rounding coordinates to get the closest node. The equilibrium between computation time and preccission was rounding to the 3th decimal (100m see above's images of the grid).

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/grid_measurement.PNG" width="50%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/grid_overview.PNG" width="50%">

&emsp;When I started to develop the model, once the grid was defined, given a point I calculated the euclidean distance to each one of the nodes in the grid. The result was too much running time (**too much obviously**). To fix this problem, the algorithm I started to follow was get round-coordinates identified nodes so that given a point I just would have to round it and found the coincidence. That's the node assigned to it. 

&emsp;With the parts of the development clear, the algorithm is simple, assign a size or a color according to the repetitions found in our data, the results are the following:
#### Density Maps Results

###### Output as a circle map

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/size_map_overview.PNG">

###### Output as a heatmap

> Zoomed-in(left), Overview(right)

> The Hotter color, the more populated zone

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/heatmap_in_detail.PNG" width="50%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/heatmap_overview.PNG" width="50%">

###### Output as a anomaly detection model

&emsp;And finally, this is why above results are in [Models section](#models) and not in visualizations. After getting this visual results, I started wondering what would happen If I scale this density map to a anomaly detection map to alert when a ship is in a non-frequently populated area (less than 5 repetitions in historical data). This are the results:

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/DENSITYMAP/model_derived_overview.PNG">

&emsp;The results are good comparing them to the heatmap and the size map (anomalies are thrown in the expected areas) but the results are not powerful enough, even though, it is a valid model to detect ships in weird positions according to historical data.

### KNN adaptation to unsupervised learning
#### About unsupervised KNN

&emsp;The notebooks related to this model are the following:

* [Main notebook with direction model finished]("https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/directional_KNN_rounding-V5.ipynb")

* [Function to export the functionalities to other features in furthermore research]("https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/generic_KNN_rounding_function.ipynb")

&emsp;Inspired by the idea of the previous simple model, I started thinking about ways to apply the grid method for anomaly detection for more useful results. Comming with the idea of storaging the most common directions for each node to detect wether a ship is facing an anomalous direction according to historical data. So, I got this map:

> <img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/directions_orientation_colors.PNG" width="5%">  This is the relation between colors and directions.   

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/directional_map_overview.PNG">

&emsp;This result was unexpectedly good and opened a lot of doors for our project from my point of view. The next steps should be look for a way to measure uniformly (a general method for all nodes) to get the density of a given direction at a certain point when consulting the historic data.

&emsp;Looking at the previous visualization there is always a predominant direction for each node so I tried to aproximate the distribution with a gaussian bell. But this was not accurate enough due to the fact that some nodes do not have just one predominant direction but two, three or even a range of them:

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/normal_distribution_aproximation.png" width="50%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/example_of_two_directions.png" width="50%">

###### Kernel Density Estimation

&emsp;Kernel dDensity Estimation or KDE is a method to approximate multimodal distributions of data by using different kinds of kernels (gaussian is one of them). In other words, is exactly what I was looking for.

&emsp;After discovering It I starded doing my research on this method and understanding It. Bassically explained It generates a combination of various normal distributions separated one from each other by a bandwith (a hyperparameter). Once I got the visually desired bandwidth, I got the following result:

<img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/KDE_distribution_aproximation.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/Visualization_of_kde_aproximation.png" width="33%"><img src="https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/example_of_two_directions.png" width="33%">

&emsp;In three dimensions, KDE fits perfectly to the historical directions for the same node (midle picture).


&emsp;As can be seen in the image before shown, the results are much better according to the same data if comparing, It only lasts to create a anomaly detection map to test this idea out. This version of the anomaly detection map includes new color codes and functionalities associated.

* ![#0000ff](https://placehold.it/15/0000ff/000000?text=+) As BLUE: Anchored ships → They should be treated differently → working on that
* ![#FFA500](https://placehold.it/15/FFA500/000000?text=+) As ORANGE: Trajectories for which we do not have enough info in its closest node to conclude anything → Can be fixed with more data
* ![#ff0000](https://placehold.it/15/ff0000/000000?text=+) As RED: Anomaly according to the historical data in the closest node.
* ![#00ff00](https://placehold.it/15/00ff00/000000?text=+) As GREEN: Normal behaviors.

&emsp;Once implemented the model, I realised that there were found some little anomalies on direction that would not make sense to alert operators for (noise). The solution for that was implementing a buffer. To get printed as RED, ORANGE or BLUE we take into account 5 steps. If within those 5 steps more than the 60% of the transitions have been labeled under the same color, the transition we are evaluating will get that color.

#### Unsupervised KNN Results

&emsp;This prototype is perfectly functional and was aproved by our product owner and it is directly related to our research question as this algorithm verifies it is possible to develop an algorithm able to read the behaviour of the ships in real time and label certain ships as an anomaly, answers what an anomaly is under the context of direction.
&emsp;Also, It is evident that this can be scaled to other features such as speed, rate of turn, etc. in futhermore research.

![](https://github.com/antoniomgf1998/Portfolio/blob/master/Unsupervised_KNN/results_KNN.PNG)

## Prototype

&emsp;Not much to say about the develop of the prototype because I have not been involved in that task but once It was functional, I contributed with Stephan to export the results of the unsupervised KNN approach and adapt the data structures to it since it was one of the selected models to compare in the research paper.

## Paper
&emsp;My contribution to the paper has been formally defining the KNN approach and write everything related to it. In our paper we compare NN approach and the named statistical approach which is the unsupervised version of KNN explained above.

### Reflection

#### Reflection on own contribution to the project

&emsp;As can be seen in this portfolio, I have been present in almost all of the main task in the project and I have tried to mantain an overview of how was everything going at every moment. In fact, this has not been tough beacuse It was a project that has motivated me and It has been a great experience to gain knowledge in a field that fits pretty much with my background studies and also with my technical interests. Along all the paragraphs above all my contributions are explained in more detail. 

&emsp;I have worked a lot on developing new models that could fit to our reaserch question and the needs of the product owner and as I was not a noob in the field and my mathematical background made me understand better the literature on the subject has led my to carry most of the technical work as It can be proved in the contents of this portfolio.

#### Reflection on own learning objectives
&emsp;My own learning objectives during this minor were get a better insight on that _black box_ proccesses which is the way ML models are frequently treated and seen. In my degree in Spain I had already had a subject about artificial intelligence so I was not starting from scratch. 
&emsp;The fact of facing a real problem, with real data  and real issues on a big project with a non trivial solution and really on the state of the art, has been really rewarding and even though I have much more to learn on data science I have discovered a field to which I would not mind to orientate my application to when I get my studies finished.

#### Evaluation on the group project as a whole

&emsp;The group as a whole has worked really well during the minor and I have learned a lot about my team-mates. We have faced uncertainty moments always with patience to finally reach really good results, taking into acount that this project has a really big scope, in just 6 months with a lot of different advanced approaches, discussing them, brainstorming and definitely helping each other with the best of everyone to get to this point.

### Conclussion

&emsp;The results obtained have been really good and nurturing on all the models tested out, specially in the case of the unsupervised KNN model. We have so been congratulated by our product owner because we reached both goals of reaching a model and implementing a prototype so even though this project's scope exceeds 6 months of teamwork, we have reached a reasonable level of performance and we can conclude that It has been successful.
